{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 20230929\n# 下面的代码不用了 直接使用git clone https://huggingface.co/THUDM/chatglm2-6b-32k 就能下载模型bin和各类附属的小文件了","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModel,AutoTokenizer\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\",trust_remote_code=True,device_map=\"auto\")\nmodel.save_pretrained(\"local_chatglm2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenzier= AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\",trust_remote_code=True)\ntokenzier.save_pretrained(\"local_chatglm2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 还有一个《先SFT再DPO_0829》 在hughug账户\n# https://www.kaggle.com/code/hughug/sft-dpo-0829#%E6%99%AE%E9%80%9Alora-%E5%BD%93ref_model=None%E6%97%B6-lora-target-modules=%5Bdefault%E3%80%82%E3%80%82%E3%80%82%5D%E5%B1%85%E7%84%B6%E4%BC%9A%E6%B7%B7%E5%85%A5%E5%A5%87%E6%80%AA%E7%9A%84default-module-%E6%88%91%E5%8F%AA%E5%A5%BD%E5%9C%A8%E5%87%BD%E6%95%B0%E4%B8%AD%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4","metadata":{}},{"cell_type":"markdown","source":"# xuming的colab演示 dpo\nhttps://colab.research.google.com/drive/1kMIe3pTec2snQvLBA00Br8ND1_zwy3Gr?usp=sharing#scrollTo=Xmltp4ILwSfu","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf MedicalGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 拉取特定分支 git clone -b ","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b medGPT_0828 https://github.com/valkryhx/MedicalGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 进入项目检查分支是否拉取正确 git status","metadata":{}},{"cell_type":"code","source":"%cd MedicalGPT\n!git status","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd MedicalGPT\n!git pull --all --force\n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 按照colab演示 开始","metadata":{}},{"cell_type":"code","source":"%ls ./data/reward/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cat ./data/reward/test.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!python dpo_training.py \\\n    --model_type bloom \\\n    --model_name_or_path merged-sft \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 50 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{}},{"cell_type":"markdown","source":"# peft==0.5.0 则accelerate必须为0.21.0 不然会报错\n# [ImportError: cannot import name 'is_npu_available' from 'accelerate.utils]\n# https://github.com/eosphoros-ai/DB-GPT-Hub/issues/42","metadata":{}},{"cell_type":"code","source":"# requirements.txt中已经是了\n#!pip install accelerate==0.21","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 下面是没有使用qlora的 能跑！","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python dpo_training.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b  \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for qlora\n!pip install bitsandbytes==0.39.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 改成 自己的dpo_train_2.py","metadata":{}},{"cell_type":"code","source":"#!pip install peft==0.5.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 不要用 dpo_train_2.py  这个无法正常运行\n# 可以用dpo_for_peftmodel.py\n# 可以用dpo_peftmode_my_style_0830.py 这是我自己的用法 能用luzi.json先传一部分参数","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python dpo_train_2.py \\\n    --qlora False \\\n    --learning_rate 2e-5 \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/MedicalGPT/outputs-dpo-0924/checkpoint-80  \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用AutoPeftModelForCausalLM的写法的dpo_peftmodel_my_style_0830.py 可以run  \n# 先不使用qlora  设置qlora=False   ,use_ref_model=True(默认)  会爆显存oom  \n# 先不使用qlora 设置qlora=False  ,use_ref_model=False ,load_in_4bit=True 会报ValueError: Target modules [] not found in the base model. Please check the target modules and try again.  这可能是因为加载的model本身就是peftmodel了 不能再量化了\n# qlora=False ,use_ref_model=False ,load_in_4bit=False 爆显存 差一点点就行\n# <font color=red>qlora=True ,use_ref_model=False ,load_in_4bit=True 可以运行 </font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_peftmodel_my_style_0830.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/MedicalGPT/outputs-dpo-0924 \\\n    --tokenizer_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用最新的将basemodel和adapter分开加载的dpo train new 脚本\n\n# 在单张A800上可以使用\n# CUDA_VISIBLE_DEVICES=0 python dpo_my_style_0926_new.py\n\n# 测试不使用--resume_from_checkpoint的效果  可以run","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model True \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用最新的将basemodel和adapter分开加载的dpo train new 脚本\n\n# 在单张A800上可以使用\n# CUDA_VISIBLE_DEVICES=0 python dpo_my_style_0926_new.py\n\n# 测试开启--resume_from_checkpoint的效果  可以run","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --resume_from_checkpoint  /kaggle/working/MedicalGPT/outputs-dpo-0924 \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model True \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --force --all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 用yunguan_langchain_0926里面的langchain格式样例跑 设置--max_source_length 1024 \\ --max_target_length 1200 \\ --gradient_accumulation_steps 4 居然可以没有oom，显存占用14.7G","metadata":{}},{"cell_type":"markdown","source":"# <font color=red>非继续训练  这里使用本地的chatglm2文件 也能运行qlora  --use_ref_model False \\\n# 注意bs=1 gradient_accumulation_steps=4   GPU0=14.5G  GPU1=9.7G</font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/local_chatglm2 \\\n    --train_file_dir ./data/yunguan_langchain_0926 \\\n    --validation_file_dir ./data/yunguan_langchain_0926 \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 1024 \\\n    --max_target_length 1200 \\\n    --output_dir output_dpo_0926_qlora \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --compute_dtype fp16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>继续训练  这里使用本地的chatglm2文件 也能运行qlora 测试  use_ref_model True\n# 注意bs=1 gradient_accumulation_steps=4   GPU0=14.5G  GPU1=9.7G</font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/local_chatglm2 \\\n    --resume_from_checkpoint /kaggle/working/MedicalGPT/output_dpo_0926_qlora/checkpoint-40 \\\n    --train_file_dir ./data/yunguan_langchain_0926 \\\n    --validation_file_dir ./data/yunguan_langchain_0926 \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples -1 \\\n    --max_eval_samples 20 \\\n    --num_train_epochs 3 \\\n    --eval_steps 10 \\\n    --save_steps 20 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 1024 \\\n    --max_target_length 1200 \\\n    --output_dir output_dpo_0926_qlora_continually \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --compute_dtype fp16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#20230929 测试chatglm2-6b-32k\n%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b-32k \\\n    --train_file_dir ./data/yunguan_langchain_0926 \\\n    --validation_file_dir ./data/yunguan_langchain_0926 \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 1024 \\\n    --max_target_length 1200 \\\n    --output_dir output_dpo_0929_qlora_32k \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --compute_dtype fp16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 为chatglm系列实现的cpu推理llama.cpp 类似的chatglm.cpp   https://github.com/li-plus/chatglm.cpp  可以试试\n\n# GGML\nhttps://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML\nhttps://github.com/ggerganov/ggml\n\n# Llama的GGML  目前最新版的已不支持ggml格式 开始支持gguf格式\nhttps://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML\n\n# GGUF 是GGML团队新开发的格式 \nhttps://huggingface.co/TheBloke/CodeLlama-7B-GGUF","metadata":{}},{"cell_type":"markdown","source":"# 试试chatglm.cpp项目 的cpu加速 （gpu加速在后面）","metadata":{}},{"cell_type":"code","source":"# 使用Python binding的方式安装  \n!CMAKE_ARGS=\"-DGGML_OPENBLAS=ON\" pip install -U chatglm-cpp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 下载项目打到本地  主要是要使用其 convert.py 文件来量化模型\n%cd /kaggle/working/\n!git clone --recursive https://github.com/li-plus/chatglm.cpp.git ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ls chatglm.cpp/\n%cd chatglm.cpp/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 chatglm_cpp/convert.py \\\n-i THUDM/chatglm2-6b \\\n-l  /kaggle/working/MedicalGPT/output_dpo_0926_qlora_continually/checkpoint-40  \\\n-t q4_0 \\\n-o chatglm-ggml.bin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#测试 stream chat   \n# CPU 1.9G 但是  kaggle上似乎是for循环还是啥 卡住","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 examples/cli_chat.py -m /kaggle/working/chatglm.cpp/chatglm-ggml.bin -i","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 测试chat # 不能在chatglm.cpp目录中测\n%cd /kaggle/working  \nimport chatglm_cpp\npipeline = chatglm_cpp.Pipeline(\"/kaggle/working/chatglm.cpp/chatglm-ggml.bin\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nt_start = time.time()\nres = pipeline.chat([\"介绍一下中秋节的来历\"])\nt_end = time.time()\nprint(res)\nt_cost = t_end - t_start\nprint(f\"t_cost={t_cost} s\")\nprint(f\"chars_per_second={len(res)/(t_cost)} c/s\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"关于 threads 的含义 是推理时使用的cpu核数  https://github.com/li-plus/chatglm.cpp/issues/62\n    examples/web_demo中有设置generation_dict 里面有设置threads\n    https://github.com/li-plus/chatglm.cpp/blob/main/examples/web_demo.py#L38","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>明天对比下ggml的gpu和 ggml的cpu 以及原始的推理速度</red>","metadata":{}},{"cell_type":"markdown","source":"# 加速后GPU推理 速度达到70char/s-80char/s , 50 tokens/s ,一个 token可以表示1-3个汉字 大概。","metadata":{}},{"cell_type":"code","source":"# 使用Python binding的方式安装   gpu加速 开启 CMAKE_ARGS=\"-DGGML_CUBLAS=ON\n!CMAKE_ARGS=\"-DGGML_CUBLAS=ON\" pip install -U chatglm-cpp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os,torch\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\ntorch.cuda.device_count() # result is1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 测试chat # 不能在chatglm.cpp目录中测\n%cd /kaggle/working  \nimport chatglm_cpp\npipeline = chatglm_cpp.Pipeline(\"/kaggle/working/chatglm.cpp/chatglm-ggml.bin\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer= AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\",trust_remote_code=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nt_start = time.time()\n#res = pipeline.chat([\"介绍一下中秋节的来历\"])\nres = pipeline.chat([\"讲一讲 圆周率如何根据级数计算\"])\nt_end = time.time()\nprint(res)\nt_cost = t_end - t_start\nprint(f\"t_cost={t_cost} s\")\nprint(f\"chars_per_second={len(res)/(t_cost)} chars/s\")\nprint(f\"tokens_per_second={len(tokenizer.encode(res))/(t_cost)} tks/s\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 普通推理速度测试","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport time\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True,device_map='auto')\nmodel = model.eval()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 下面的普通推理 非量化 占了两块卡 每块显存6-7G\n# t_cost=20.126879692077637 s\n# chars_per_second=25.0411393972005 chars/s\n# tokens_per_second=17.588419338509876 tks/s","metadata":{}},{"cell_type":"code","source":"t_start = time.time()\nres, history = model.chat(tokenizer, \"讲一讲 圆周率如何根据级数计算\", history=[])\nt_end = time.time()\nprint(res)\nt_cost = t_end - t_start\nprint(f\"t_cost={t_cost} s\")\nprint(f\"chars_per_second={len(res)/(t_cost)} chars/s\")\nprint(f\"tokens_per_second={len(tokenizer.encode(res))/(t_cost)} tks/s\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用 bitsandbytes 的NF4 量化 2gpu 每块显卡3-3.5G \n# t_cost=25.050681114196777 s\n# chars_per_second=23.432496598558913 chars/s\n# tokens_per_second=17.44463545753023 tks/s","metadata":{}},{"cell_type":"code","source":"# 使用 bitsandbytes 的NF4 量化\n\n!pip install accelerate==0.21.0\n!pip install bitsandbytes==0.41.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\nfrom transformers import AutoTokenizer, AutoModel\nimport time\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n\nq_config = BitsAndBytesConfig(load_in_4bit=True,\n                                  bnb_4bit_quant_type='nf4',\n                                  bnb_4bit_use_double_quant=True,\n                                  bnb_4bit_compute_dtype=torch.float16)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True,device_map='auto',quantization_config=q_config)\nmodel = model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_start = time.time()\nres, history = model.chat(tokenizer, \"讲一讲 圆周率如何根据级数计算\", history=[])\nt_end = time.time()\nprint(res)\nt_cost = t_end - t_start\nprint(f\"t_cost={t_cost} s\")\nprint(f\"chars_per_second={len(res)/(t_cost)} chars/s\")\nprint(f\"tokens_per_second={len(tokenizer.encode(res))/(t_cost)} tks/s\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用 bitsandbytes 的NF4 量化 1 gpu 显卡5.5G \n# t_cost=13.949055194854736 s\n# chars_per_second=27.241988413678886 chars/s\n# tokens_per_second=18.782634116799652 tks/s","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\nfrom transformers import AutoTokenizer, AutoModel\nimport time\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n\nq_config = BitsAndBytesConfig(load_in_4bit=True,\n                                  bnb_4bit_quant_type='nf4',\n                                  bnb_4bit_use_double_quant=True,\n                                  bnb_4bit_compute_dtype=torch.float16)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True,device_map={\"\":1},quantization_config=q_config)\nmodel = model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T05:22:54.337159Z","iopub.execute_input":"2023-09-30T05:22:54.337555Z","iopub.status.idle":"2023-09-30T05:48:29.142082Z","shell.execute_reply.started":"2023-09-30T05:22:54.337523Z","shell.execute_reply":"2023-09-30T05:48:29.141000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_start = time.time()\nres, history = model.chat(tokenizer, \"讲一讲 圆周率如何根据级数计算\", history=[])\nt_end = time.time()\nprint(res)\nt_cost = t_end - t_start\nprint(f\"t_cost={t_cost} s\")\nprint(f\"chars_per_second={len(res)/(t_cost)} chars/s\")\nprint(f\"tokens_per_second={len(tokenizer.encode(res))/(t_cost)} tks/s\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T06:00:15.037006Z","iopub.execute_input":"2023-09-30T06:00:15.037734Z","iopub.status.idle":"2023-09-30T06:00:33.062333Z","shell.execute_reply.started":"2023-09-30T06:00:15.037701Z","shell.execute_reply":"2023-09-30T06:00:33.061222Z"},"trusted":true},"execution_count":null,"outputs":[]}]}