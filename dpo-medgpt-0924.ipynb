{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModel,AutoTokenizer\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\",trust_remote_code=True,device_map=\"auto\")\nmodel.save_pretrained(\"local_chatglm2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenzier= AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\",trust_remote_code=True)\ntokenzier.save_pretrained(\"local_chatglm2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 还有一个《先SFT再DPO_0829》 在hughug账户\n# https://www.kaggle.com/code/hughug/sft-dpo-0829#%E6%99%AE%E9%80%9Alora-%E5%BD%93ref_model=None%E6%97%B6-lora-target-modules=%5Bdefault%E3%80%82%E3%80%82%E3%80%82%5D%E5%B1%85%E7%84%B6%E4%BC%9A%E6%B7%B7%E5%85%A5%E5%A5%87%E6%80%AA%E7%9A%84default-module-%E6%88%91%E5%8F%AA%E5%A5%BD%E5%9C%A8%E5%87%BD%E6%95%B0%E4%B8%AD%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4","metadata":{}},{"cell_type":"markdown","source":"# xuming的colab演示 dpo\nhttps://colab.research.google.com/drive/1kMIe3pTec2snQvLBA00Br8ND1_zwy3Gr?usp=sharing#scrollTo=Xmltp4ILwSfu","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-09-26T12:25:38.237171Z","iopub.execute_input":"2023-09-26T12:25:38.238155Z","iopub.status.idle":"2023-09-26T12:25:38.252740Z","shell.execute_reply.started":"2023-09-26T12:25:38.238109Z","shell.execute_reply":"2023-09-26T12:25:38.251338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf MedicalGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 拉取特定分支 git clone -b ","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b medGPT_0828 https://github.com/valkryhx/MedicalGPT","metadata":{"execution":{"iopub.status.busy":"2023-09-26T12:26:14.792044Z","iopub.execute_input":"2023-09-26T12:26:14.792416Z","iopub.status.idle":"2023-09-26T12:26:15.803539Z","shell.execute_reply.started":"2023-09-26T12:26:14.792385Z","shell.execute_reply":"2023-09-26T12:26:15.802319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 进入项目检查分支是否拉取正确 git status","metadata":{}},{"cell_type":"code","source":"%cd MedicalGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-09-26T12:26:21.876901Z","iopub.execute_input":"2023-09-26T12:26:21.877289Z","iopub.status.idle":"2023-09-26T12:26:22.939854Z","shell.execute_reply.started":"2023-09-26T12:26:21.877258Z","shell.execute_reply":"2023-09-26T12:26:22.938644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd MedicalGPT\n!git pull --all --force\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-26T12:27:04.137863Z","iopub.execute_input":"2023-09-26T12:27:04.138869Z","iopub.status.idle":"2023-09-26T12:27:44.818933Z","shell.execute_reply.started":"2023-09-26T12:27:04.138833Z","shell.execute_reply":"2023-09-26T12:27:44.817620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 按照colab演示 开始","metadata":{}},{"cell_type":"code","source":"%ls ./data/reward/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cat ./data/reward/test.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!python dpo_training.py \\\n    --model_type bloom \\\n    --model_name_or_path merged-sft \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 50 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{}},{"cell_type":"markdown","source":"# peft==0.5.0 则accelerate必须为0.21.0 不然会报错\n# [ImportError: cannot import name 'is_npu_available' from 'accelerate.utils]\n# https://github.com/eosphoros-ai/DB-GPT-Hub/issues/42","metadata":{}},{"cell_type":"code","source":"# requirements.txt中已经是了\n#!pip install accelerate==0.21","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 下面是没有使用qlora的 能跑！","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python dpo_training.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b  \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for qlora\n!pip install bitsandbytes==0.39.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 改成 自己的dpo_train_2.py","metadata":{}},{"cell_type":"code","source":"#!pip install peft==0.5.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 不要用 dpo_train_2.py  这个无法正常运行\n# 可以用dpo_for_peftmodel.py\n# 可以用dpo_peftmode_my_style_0830.py 这是我自己的用法 能用luzi.json先传一部分参数","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python dpo_train_2.py \\\n    --qlora False \\\n    --learning_rate 2e-5 \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/MedicalGPT/outputs-dpo-0924/checkpoint-80  \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用AutoPeftModelForCausalLM的写法的dpo_peftmodel_my_style_0830.py 可以run  \n# 先不使用qlora  设置qlora=False   ,use_ref_model=True(默认)  会爆显存oom  \n# 先不使用qlora 设置qlora=False  ,use_ref_model=False ,load_in_4bit=True 会报ValueError: Target modules [] not found in the base model. Please check the target modules and try again.  这可能是因为加载的model本身就是peftmodel了 不能再量化了\n# qlora=False ,use_ref_model=False ,load_in_4bit=False 爆显存 差一点点就行\n# <font color=red>qlora=True ,use_ref_model=False ,load_in_4bit=True 可以运行 </font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_peftmodel_my_style_0830.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/MedicalGPT/outputs-dpo-0924 \\\n    --tokenizer_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用最新的将basemodel和adapter分开加载的dpo train new 脚本\n\n# 在单张A800上可以使用\n# CUDA_VISIBLE_DEVICES=0 python dpo_my_style_0926_new.py\n\n# 测试不使用--resume_from_checkpoint的效果  可以run","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model True \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用最新的将basemodel和adapter分开加载的dpo train new 脚本\n\n# 在单张A800上可以使用\n# CUDA_VISIBLE_DEVICES=0 python dpo_my_style_0926_new.py\n\n# 测试开启--resume_from_checkpoint的效果  可以run","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --resume_from_checkpoint  /kaggle/working/MedicalGPT/outputs-dpo-0924 \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model True \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --force --all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 用yunguan_langchain_0926里面的langchain格式样例跑 设置--max_source_length 1024 \\ --max_target_length 1200 \\ --gradient_accumulation_steps 4 居然可以没有oom，显存占用14.7G","metadata":{}},{"cell_type":"markdown","source":"# <font color=red>非继续训练  这里使用本地的chatglm2文件 也能运行qlora  --use_ref_model False \\\n# 注意bs=1 gradient_accumulation_steps=4   GPU0=14.5G  GPU1=9.7G</font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/local_chatglm2 \\\n    --train_file_dir ./data/yunguan_langchain_0926 \\\n    --validation_file_dir ./data/yunguan_langchain_0926 \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 1024 \\\n    --max_target_length 1200 \\\n    --output_dir output_dpo_0926_qlora \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --compute_dtype fp16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json","metadata":{"execution":{"iopub.status.busy":"2023-09-26T12:49:24.339013Z","iopub.execute_input":"2023-09-26T12:49:24.340254Z","iopub.status.idle":"2023-09-26T14:13:37.983755Z","shell.execute_reply.started":"2023-09-26T12:49:24.340211Z","shell.execute_reply":"2023-09-26T14:13:37.982192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>继续训练  这里使用本地的chatglm2文件 也能运行qlora 测试  use_ref_model True\n# 注意bs=1 gradient_accumulation_steps=4   GPU0=14.5G  GPU1=9.7G</font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/local_chatglm2 \\\n    --resume_from_checkpoint /kaggle/working/MedicalGPT/output_dpo_0926_qlora/checkpoint-40 \\\n    --train_file_dir ./data/yunguan_langchain_0926 \\\n    --validation_file_dir ./data/yunguan_langchain_0926 \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples -1 \\\n    --max_eval_samples 20 \\\n    --num_train_epochs 3 \\\n    --eval_steps 10 \\\n    --save_steps 20 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 1024 \\\n    --max_target_length 1200 \\\n    --output_dir output_dpo_0926_qlora_continually \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --compute_dtype fp16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:27:23.057059Z","iopub.execute_input":"2023-09-26T14:27:23.057489Z","iopub.status.idle":"2023-09-26T15:57:40.652674Z","shell.execute_reply.started":"2023-09-26T14:27:23.057454Z","shell.execute_reply":"2023-09-26T15:57:40.651250Z"},"trusted":true},"execution_count":null,"outputs":[]}]}