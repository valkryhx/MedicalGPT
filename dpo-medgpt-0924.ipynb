{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-28T11:21:55.806114Z","iopub.execute_input":"2023-08-28T11:21:55.806472Z","iopub.status.idle":"2023-08-28T11:21:55.820875Z","shell.execute_reply.started":"2023-08-28T11:21:55.806441Z","shell.execute_reply":"2023-08-28T11:21:55.819398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 还有一个《先SFT再DPO_0829》 在hughug账户\n# https://www.kaggle.com/code/hughug/sft-dpo-0829#%E6%99%AE%E9%80%9Alora-%E5%BD%93ref_model=None%E6%97%B6-lora-target-modules=%5Bdefault%E3%80%82%E3%80%82%E3%80%82%5D%E5%B1%85%E7%84%B6%E4%BC%9A%E6%B7%B7%E5%85%A5%E5%A5%87%E6%80%AA%E7%9A%84default-module-%E6%88%91%E5%8F%AA%E5%A5%BD%E5%9C%A8%E5%87%BD%E6%95%B0%E4%B8%AD%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4","metadata":{}},{"cell_type":"markdown","source":"# xuming的colab演示 dpo\nhttps://colab.research.google.com/drive/1kMIe3pTec2snQvLBA00Br8ND1_zwy3Gr?usp=sharing#scrollTo=Xmltp4ILwSfu","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-08-28T11:59:45.210146Z","iopub.execute_input":"2023-08-28T11:59:45.211238Z","iopub.status.idle":"2023-08-28T11:59:45.219558Z","shell.execute_reply.started":"2023-08-28T11:59:45.211188Z","shell.execute_reply":"2023-08-28T11:59:45.218456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf MedicalGPT","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:00:02.306733Z","iopub.execute_input":"2023-08-28T12:00:02.307657Z","iopub.status.idle":"2023-08-28T12:00:03.307443Z","shell.execute_reply.started":"2023-08-28T12:00:02.307611Z","shell.execute_reply":"2023-08-28T12:00:03.306114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 拉取特定分支 git clone -b ","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b medGPT_0828 https://github.com/valkryhx/MedicalGPT","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:07:19.387032Z","iopub.execute_input":"2023-09-24T13:07:19.387351Z","iopub.status.idle":"2023-09-24T13:07:20.406777Z","shell.execute_reply.started":"2023-09-24T13:07:19.387322Z","shell.execute_reply":"2023-09-24T13:07:20.405453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 进入项目检查分支是否拉取正确 git status","metadata":{}},{"cell_type":"code","source":"%cd MedicalGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:08:05.588460Z","iopub.execute_input":"2023-09-24T13:08:05.588909Z","iopub.status.idle":"2023-09-24T13:08:06.609020Z","shell.execute_reply.started":"2023-09-24T13:08:05.588873Z","shell.execute_reply":"2023-09-24T13:08:06.607798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd MedicalGPT\n!git pull --all --force\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:45:10.916282Z","iopub.execute_input":"2023-09-24T14:45:10.916731Z","iopub.status.idle":"2023-09-24T14:45:39.304694Z","shell.execute_reply.started":"2023-09-24T14:45:10.916685Z","shell.execute_reply":"2023-09-24T14:45:39.303259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 按照colab演示 开始","metadata":{}},{"cell_type":"code","source":"%ls ./data/reward/","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:10:51.898426Z","iopub.execute_input":"2023-09-24T13:10:51.898952Z","iopub.status.idle":"2023-09-24T13:10:53.089880Z","shell.execute_reply.started":"2023-09-24T13:10:51.898904Z","shell.execute_reply":"2023-09-24T13:10:53.088537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cat ./data/reward/test.json","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:05:01.202980Z","iopub.execute_input":"2023-08-28T12:05:01.203370Z","iopub.status.idle":"2023-08-28T12:05:01.208089Z","shell.execute_reply.started":"2023-08-28T12:05:01.203338Z","shell.execute_reply":"2023-08-28T12:05:01.207080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!python dpo_training.py \\\n    --model_type bloom \\\n    --model_name_or_path merged-sft \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 50 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{}},{"cell_type":"markdown","source":"# peft==0.5.0 则accelerate必须为0.21.0 不然会报错\n# [ImportError: cannot import name 'is_npu_available' from 'accelerate.utils]\n# https://github.com/eosphoros-ai/DB-GPT-Hub/issues/42","metadata":{}},{"cell_type":"code","source":"# requirements.txt中已经是了\n#!pip install accelerate==0.21","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:11:25.943922Z","iopub.execute_input":"2023-09-24T13:11:25.945017Z","iopub.status.idle":"2023-09-24T13:11:25.949727Z","shell.execute_reply.started":"2023-09-24T13:11:25.944971Z","shell.execute_reply":"2023-09-24T13:11:25.948565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 下面是没有使用qlora的 能跑！","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python dpo_training.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b  \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.busy":"2023-09-24T13:13:57.965299Z","iopub.execute_input":"2023-09-24T13:13:57.965689Z","iopub.status.idle":"2023-09-24T13:34:58.695752Z","shell.execute_reply.started":"2023-09-24T13:13:57.965650Z","shell.execute_reply":"2023-09-24T13:34:58.694228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for qlora\n!pip install bitsandbytes==0.39.0","metadata":{"execution":{"iopub.status.busy":"2023-08-28T13:37:19.570633Z","iopub.execute_input":"2023-08-28T13:37:19.571021Z","iopub.status.idle":"2023-08-28T13:37:34.650255Z","shell.execute_reply.started":"2023-08-28T13:37:19.570989Z","shell.execute_reply":"2023-08-28T13:37:34.649029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 改成 自己的dpo_train_2.py","metadata":{}},{"cell_type":"code","source":"#!pip install peft==0.5.0","metadata":{"execution":{"iopub.status.busy":"2023-09-24T14:05:57.853648Z","iopub.execute_input":"2023-09-24T14:05:57.854843Z","iopub.status.idle":"2023-09-24T14:06:12.145951Z","shell.execute_reply.started":"2023-09-24T14:05:57.854782Z","shell.execute_reply":"2023-09-24T14:06:12.144708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 不要用 dpo_train_2.py  这个无法正常运行\n# 可以用dpo_for_peftmodel.py\n# 可以用dpo_peftmode_my_style_0830.py 这是我自己的用法 能用luzi.json先传一部分参数","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python dpo_train_2.py \\\n    --qlora False \\\n    --learning_rate 2e-5 \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/MedicalGPT/outputs-dpo-0924/checkpoint-80  \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用dpo_peftmodel_my_style_0830.py 才对  \n# 先不使用qlora  设置qlora=False   ,use_ref_model=True(默认)  会爆显存oom  \n# 先不使用qlora 设置qlora=False  ,use_ref_model=False ,load_in_4bit=True 会报ValueError: Target modules [] not found in the base model. Please check the target modules and try again.  这可能是因为加载的model本身就是peftmodel了 不能再量化了\n# qlora=False ,use_ref_model=False ,load_in_4bit=False 爆显存 差一点点就行\n# <font color=red>qlora=True ,use_ref_model=False ,load_in_4bit=True 可以运行 </font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_peftmodel_my_style_0830.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/MedicalGPT/outputs-dpo-0924 \\\n    --tokenizer_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T15:41:15.051357Z","iopub.execute_input":"2023-09-24T15:41:15.051856Z","iopub.status.idle":"2023-09-24T16:25:18.971088Z","shell.execute_reply.started":"2023-09-24T15:41:15.051807Z","shell.execute_reply":"2023-09-24T16:25:18.969267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:22:56.671868Z","iopub.execute_input":"2023-08-28T12:22:56.672354Z","iopub.status.idle":"2023-08-28T12:23:12.312021Z","shell.execute_reply.started":"2023-08-28T12:22:56.672316Z","shell.execute_reply":"2023-08-28T12:23:12.310852Z"},"trusted":true},"execution_count":null,"outputs":[]}]}