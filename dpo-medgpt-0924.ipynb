{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-29T15:27:56.838119Z","iopub.execute_input":"2023-09-29T15:27:56.838494Z","iopub.status.idle":"2023-09-29T15:27:56.875591Z","shell.execute_reply.started":"2023-09-29T15:27:56.838462Z","shell.execute_reply":"2023-09-29T15:27:56.874003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 20230929\n# 下面的代码不用了 直接使用git clone https://huggingface.co/THUDM/chatglm2-6b-32k 就能下载模型bin和各类附属的小文件了","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModel,AutoTokenizer\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\",trust_remote_code=True,device_map=\"auto\")\nmodel.save_pretrained(\"local_chatglm2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenzier= AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\",trust_remote_code=True)\ntokenzier.save_pretrained(\"local_chatglm2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 还有一个《先SFT再DPO_0829》 在hughug账户\n# https://www.kaggle.com/code/hughug/sft-dpo-0829#%E6%99%AE%E9%80%9Alora-%E5%BD%93ref_model=None%E6%97%B6-lora-target-modules=%5Bdefault%E3%80%82%E3%80%82%E3%80%82%5D%E5%B1%85%E7%84%B6%E4%BC%9A%E6%B7%B7%E5%85%A5%E5%A5%87%E6%80%AA%E7%9A%84default-module-%E6%88%91%E5%8F%AA%E5%A5%BD%E5%9C%A8%E5%87%BD%E6%95%B0%E4%B8%AD%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4","metadata":{}},{"cell_type":"markdown","source":"# xuming的colab演示 dpo\nhttps://colab.research.google.com/drive/1kMIe3pTec2snQvLBA00Br8ND1_zwy3Gr?usp=sharing#scrollTo=Xmltp4ILwSfu","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-09-26T12:25:38.237171Z","iopub.execute_input":"2023-09-26T12:25:38.238155Z","iopub.status.idle":"2023-09-26T12:25:38.252740Z","shell.execute_reply.started":"2023-09-26T12:25:38.238109Z","shell.execute_reply":"2023-09-26T12:25:38.251338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf MedicalGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 拉取特定分支 git clone -b ","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b medGPT_0828 https://github.com/valkryhx/MedicalGPT","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:28:19.613976Z","iopub.execute_input":"2023-09-29T15:28:19.615249Z","iopub.status.idle":"2023-09-29T15:28:20.763958Z","shell.execute_reply.started":"2023-09-29T15:28:19.615206Z","shell.execute_reply":"2023-09-29T15:28:20.762251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 进入项目检查分支是否拉取正确 git status","metadata":{}},{"cell_type":"code","source":"%cd MedicalGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:28:23.641244Z","iopub.execute_input":"2023-09-29T15:28:23.641768Z","iopub.status.idle":"2023-09-29T15:28:24.962591Z","shell.execute_reply.started":"2023-09-29T15:28:23.641723Z","shell.execute_reply":"2023-09-29T15:28:24.961419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd MedicalGPT\n!git pull --all --force\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:28:29.306602Z","iopub.execute_input":"2023-09-29T15:28:29.307035Z","iopub.status.idle":"2023-09-29T15:29:03.090366Z","shell.execute_reply.started":"2023-09-29T15:28:29.306999Z","shell.execute_reply":"2023-09-29T15:29:03.088877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 按照colab演示 开始","metadata":{}},{"cell_type":"code","source":"%ls ./data/reward/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cat ./data/reward/test.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!python dpo_training.py \\\n    --model_type bloom \\\n    --model_name_or_path merged-sft \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 50 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{}},{"cell_type":"markdown","source":"# peft==0.5.0 则accelerate必须为0.21.0 不然会报错\n# [ImportError: cannot import name 'is_npu_available' from 'accelerate.utils]\n# https://github.com/eosphoros-ai/DB-GPT-Hub/issues/42","metadata":{}},{"cell_type":"code","source":"# requirements.txt中已经是了\n#!pip install accelerate==0.21","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 下面是没有使用qlora的 能跑！","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python dpo_training.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b  \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for qlora\n!pip install bitsandbytes==0.39.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 改成 自己的dpo_train_2.py","metadata":{}},{"cell_type":"code","source":"#!pip install peft==0.5.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 不要用 dpo_train_2.py  这个无法正常运行\n# 可以用dpo_for_peftmodel.py\n# 可以用dpo_peftmode_my_style_0830.py 这是我自己的用法 能用luzi.json先传一部分参数","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python dpo_train_2.py \\\n    --qlora False \\\n    --learning_rate 2e-5 \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/MedicalGPT/outputs-dpo-0924/checkpoint-80  \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --per_device_train_batch_size 3 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 10 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 128 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用AutoPeftModelForCausalLM的写法的dpo_peftmodel_my_style_0830.py 可以run  \n# 先不使用qlora  设置qlora=False   ,use_ref_model=True(默认)  会爆显存oom  \n# 先不使用qlora 设置qlora=False  ,use_ref_model=False ,load_in_4bit=True 会报ValueError: Target modules [] not found in the base model. Please check the target modules and try again.  这可能是因为加载的model本身就是peftmodel了 不能再量化了\n# qlora=False ,use_ref_model=False ,load_in_4bit=False 爆显存 差一点点就行\n# <font color=red>qlora=True ,use_ref_model=False ,load_in_4bit=True 可以运行 </font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_peftmodel_my_style_0830.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/MedicalGPT/outputs-dpo-0924 \\\n    --tokenizer_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用最新的将basemodel和adapter分开加载的dpo train new 脚本\n\n# 在单张A800上可以使用\n# CUDA_VISIBLE_DEVICES=0 python dpo_my_style_0926_new.py\n\n# 测试不使用--resume_from_checkpoint的效果  可以run","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model True \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用最新的将basemodel和adapter分开加载的dpo train new 脚本\n\n# 在单张A800上可以使用\n# CUDA_VISIBLE_DEVICES=0 python dpo_my_style_0926_new.py\n\n# 测试开启--resume_from_checkpoint的效果  可以run","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --resume_from_checkpoint  /kaggle/working/MedicalGPT/outputs-dpo-0924 \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model True \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-0924-v2-no-qlora \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json \\\n    --compute_dtype fp16 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --force --all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 用yunguan_langchain_0926里面的langchain格式样例跑 设置--max_source_length 1024 \\ --max_target_length 1200 \\ --gradient_accumulation_steps 4 居然可以没有oom，显存占用14.7G","metadata":{}},{"cell_type":"markdown","source":"# <font color=red>非继续训练  这里使用本地的chatglm2文件 也能运行qlora  --use_ref_model False \\\n# 注意bs=1 gradient_accumulation_steps=4   GPU0=14.5G  GPU1=9.7G</font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/local_chatglm2 \\\n    --train_file_dir ./data/yunguan_langchain_0926 \\\n    --validation_file_dir ./data/yunguan_langchain_0926 \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 1024 \\\n    --max_target_length 1200 \\\n    --output_dir output_dpo_0926_qlora \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --compute_dtype fp16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json","metadata":{"execution":{"iopub.status.busy":"2023-09-26T12:49:24.339013Z","iopub.execute_input":"2023-09-26T12:49:24.340254Z","iopub.status.idle":"2023-09-26T14:13:37.983755Z","shell.execute_reply.started":"2023-09-26T12:49:24.340211Z","shell.execute_reply":"2023-09-26T14:13:37.982192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>继续训练  这里使用本地的chatglm2文件 也能运行qlora 测试  use_ref_model True\n# 注意bs=1 gradient_accumulation_steps=4   GPU0=14.5G  GPU1=9.7G</font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/local_chatglm2 \\\n    --resume_from_checkpoint /kaggle/working/MedicalGPT/output_dpo_0926_qlora/checkpoint-40 \\\n    --train_file_dir ./data/yunguan_langchain_0926 \\\n    --validation_file_dir ./data/yunguan_langchain_0926 \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples -1 \\\n    --max_eval_samples 20 \\\n    --num_train_epochs 3 \\\n    --eval_steps 10 \\\n    --save_steps 20 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 1024 \\\n    --max_target_length 1200 \\\n    --output_dir output_dpo_0926_qlora_continually \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --compute_dtype fp16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json","metadata":{"execution":{"iopub.status.busy":"2023-09-26T14:27:23.057059Z","iopub.execute_input":"2023-09-26T14:27:23.057489Z","iopub.status.idle":"2023-09-26T15:57:40.652674Z","shell.execute_reply.started":"2023-09-26T14:27:23.057454Z","shell.execute_reply":"2023-09-26T15:57:40.651250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#20230929 测试chatglm2-6b-32k\n%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_my_style_0926_new.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b-32k \\\n    --train_file_dir ./data/yunguan_langchain_0926 \\\n    --validation_file_dir ./data/yunguan_langchain_0926 \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --use_ref_model False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 1024 \\\n    --max_target_length 1200 \\\n    --output_dir output_dpo_0929_qlora_32k \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --compute_dtype fp16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache \\\n    --train_args_json luzi.json","metadata":{"execution":{"iopub.status.busy":"2023-09-29T10:08:38.241883Z","iopub.execute_input":"2023-09-29T10:08:38.242317Z","iopub.status.idle":"2023-09-29T11:28:28.428388Z","shell.execute_reply.started":"2023-09-29T10:08:38.242278Z","shell.execute_reply":"2023-09-29T11:28:28.427038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 为chatglm系列实现的cpu推理llama.cpp 类似的chatglm.cpp   https://github.com/li-plus/chatglm.cpp  可以试试\n\n# GGML\nhttps://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML\nhttps://github.com/ggerganov/ggml\n\n# Llama的GGML  目前最新版的已不支持ggml格式 开始支持gguf格式\nhttps://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML\n\n# GGUF 是GGML团队新开发的格式 \nhttps://huggingface.co/TheBloke/CodeLlama-7B-GGUF","metadata":{}},{"cell_type":"markdown","source":"# 试试chatglm.cpp项目","metadata":{}},{"cell_type":"code","source":"# 使用Python binding的方式安装 \n!CMAKE_ARGS=\"-DGGML_OPENBLAS=ON\" pip install -U chatglm-cpp","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:32:22.991401Z","iopub.execute_input":"2023-09-29T15:32:22.991761Z","iopub.status.idle":"2023-09-29T15:35:14.080547Z","shell.execute_reply.started":"2023-09-29T15:32:22.991735Z","shell.execute_reply":"2023-09-29T15:35:14.079042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 下载项目打到本地  主要是要使用其 convert.py 文件来量化模型\n%cd /kaggle/working/\n!git clone --recursive https://github.com/li-plus/chatglm.cpp.git ","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:38:33.749357Z","iopub.execute_input":"2023-09-29T15:38:33.751274Z","iopub.status.idle":"2023-09-29T15:38:34.914544Z","shell.execute_reply.started":"2023-09-29T15:38:33.751171Z","shell.execute_reply":"2023-09-29T15:38:34.913410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ls chatglm.cpp/\n%cd chatglm.cpp/","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:39:11.600681Z","iopub.execute_input":"2023-09-29T15:39:11.601101Z","iopub.status.idle":"2023-09-29T15:39:11.607654Z","shell.execute_reply.started":"2023-09-29T15:39:11.601068Z","shell.execute_reply":"2023-09-29T15:39:11.606889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 chatglm_cpp/convert.py \\\n-i THUDM/chatglm2-6b \\\n-l  /kaggle/working/MedicalGPT/output_dpo_0926_qlora_continually/checkpoint-40  \\\n-t q4_0 \\\n-o chatglm-ggml.bin","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:54:18.703138Z","iopub.execute_input":"2023-09-29T15:54:18.703616Z","iopub.status.idle":"2023-09-29T15:57:50.612509Z","shell.execute_reply.started":"2023-09-29T15:54:18.703574Z","shell.execute_reply":"2023-09-29T15:57:50.611369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:02:04.360526Z","iopub.execute_input":"2023-09-29T16:02:04.362331Z","iopub.status.idle":"2023-09-29T16:02:04.373937Z","shell.execute_reply.started":"2023-09-29T16:02:04.362262Z","shell.execute_reply":"2023-09-29T16:02:04.372789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#测试 stream chat   \n# CPU 1.9G 但是  kaggle上似乎是for循环还是啥 卡住","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 examples/cli_chat.py -m /kaggle/working/chatglm.cpp/chatglm-ggml.bin -i","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:03:22.323942Z","iopub.execute_input":"2023-09-29T16:03:22.324380Z","iopub.status.idle":"2023-09-29T16:04:14.400953Z","shell.execute_reply.started":"2023-09-29T16:03:22.324350Z","shell.execute_reply":"2023-09-29T16:04:14.399848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 测试chat # 不能在chatglm.cpp目录中测\n%cd /kaggle/working  \nimport chatglm_cpp\npipeline = chatglm_cpp.Pipeline(\"/kaggle/working/chatglm.cpp/chatglm-ggml.bin\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:12:53.289334Z","iopub.execute_input":"2023-09-29T16:12:53.289732Z","iopub.status.idle":"2023-09-29T16:12:53.366014Z","shell.execute_reply.started":"2023-09-29T16:12:53.289699Z","shell.execute_reply":"2023-09-29T16:12:53.364766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nt_start = time.time()\nres = pipeline.chat([\"介绍一下中秋节的来历\"])\nt_end = time.time()\nprint(res)\nt_cost = t_end - t_start\nprint(f\"t_cost={t_cost} s\")\nprint(f\"chars_per_second={len(res)/(t_cost)} c/s\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T16:39:26.644865Z","iopub.execute_input":"2023-09-29T16:39:26.645546Z","iopub.status.idle":"2023-09-29T16:40:54.256048Z","shell.execute_reply.started":"2023-09-29T16:39:26.645499Z","shell.execute_reply":"2023-09-29T16:40:54.255039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"关于 threads 的含义 是推理时使用的cpu核数  https://github.com/li-plus/chatglm.cpp/issues/62\n    examples/web_demo中有设置generation_dict 里面有设置threads\n    https://github.com/li-plus/chatglm.cpp/blob/main/examples/web_demo.py#L38","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>明天对比下ggml的gpu和 ggml的cpu 以及原始的推理速度</red>","metadata":{}}]}